{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All scraped data saved to scraped_data_all_pages.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_page_data(page_number):\n",
    "    url = f\"http://lucbat.com/index.php?tab=cat&id=1&PageNo={page_number}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage for PageNo: {page_number}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    protop_lists = soup.find_all('div', class_='protop_list')\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for protop_list in protop_lists:\n",
    "\n",
    "        newstitle = protop_list.find('div', class_='newstitle')\n",
    "        a_tag = newstitle.find('a')\n",
    "        href = a_tag.get('href') if a_tag else None\n",
    "        full_url = f'http://lucbat.com/{href}' if href else None\n",
    "        content = a_tag.text.strip() if a_tag else None\n",
    "        data.append({'url': full_url, 'title': content})\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_data = []\n",
    "    for x in range(1, 1115):\n",
    "        \n",
    "        scraped_data = scrape_page_data(x)\n",
    "        all_data.extend(scraped_data)\n",
    "\n",
    "    with open('scraped_data_all_pages.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['url', 'title']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for item in all_data:\n",
    "            writer.writerow(item)\n",
    "    \n",
    "    print(\"All scraped data saved to scraped_data_all_pages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc tệp CSV vào DataFrame\n",
    "df = pd.read_csv('ten_file_moi.csv')  # Thay 'ten_file.csv' bằng tên thực sự của tệp CSV của bạn\n",
    "\n",
    "# Xử lý cột 'title' để loại bỏ phần từ khoảng trắng đầu tiên đến dấu ngoặc đầu tiên và dấu ngoặc cuối cùng\n",
    "df['title'] = df['title'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Ghi DataFrame đã được chỉnh sửa vào một tệp CSV mới hoặc cập nhật tệp gốc\n",
    "df.to_csv('ten_file_moi.csv', index=False)  # Thay 'ten_file_moi.csv' bằng tên bạn muốn đặt cho tệp kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Duyên thơ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Liên Vũ\n",
      "\n",
      "\n",
      "\n",
      "Dòng sông xanh thẳm chiều rơi\n",
      "Bơ vơ cánh nhạn giữa trời đơn côi\n",
      "Con thuyền lẻ bóng trôi xuôi\n",
      "Mịt mù sông nước... vợi vơi nỗi lòng...\n",
      "\n",
      "Nhìn trời, mây, nước tang bồng\n",
      "Hoàng hôn đã phủ sắc hồng bến xưa\n",
      "Mái chèo vỗ nước, đong đưa\n",
      "Tiếng con chim lạc... vẫn chưa thấy bầy.\n",
      "\n",
      "Vẳng câu \"giã bạn\" đâu đây\n",
      "Miếng trầu trao gửi, chia tay... ta về\n",
      "Mang theo chỉ một câu thề\n",
      "Mà nay gió thổi trôi về phương nao??\n",
      "\n",
      "Mênh mang.. bến nước xôn xao\n",
      "Thuyền nan mái đẩy sóng dào dạt xô\n",
      "Chiều rơi.. đến tận hư vô\n",
      "Hoàng hôn đuổi sóng nhấp nhô bồng bềnh.\n",
      "\n",
      "L.V\n",
      "\n",
      "\n",
      "\n",
      "Tác giả: Liên Vũ\n",
      "Thành Phố Hà Nội\n"
     ]
    }
   ],
   "source": [
    "url = \"http://lucbat.com/news.php?id=22948\"\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the div with class 'protop_list'\n",
    "    div_element = soup.find('div', {'class': 'protop_list'})\n",
    "\n",
    "    # If the div is found, find all p tags with class 'MsoNormal'\n",
    "    if div_element:\n",
    "        p_elements = div_element.find_all('p', {'class': 'MsoNormal'})\n",
    "\n",
    "        # Loop through each p element and print its text content\n",
    "        for p in p_elements:\n",
    "            print(p.get_text(separator='\\n', strip=True))\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tầm vươn thế đứng vin trời\n",
      "Chốn quê dựng lũy chắn đời bão giông\n",
      "Bình minh nâng mặt trời hồng\n",
      "Chiều nghiêng ngơ ngẩn hẹn trông người về\n",
      "\n",
      "\n",
      "Bờ tre treo bóng trăng thề\n",
      "Đêm dài sáo vọng tình quê mặn nồng\n",
      "Bao lần là mác là chông\n",
      "Bao đời ngọn lả cành cong đợi cò\n",
      "\n",
      "\n",
      "Trời xui bão dạt sóng xô\n",
      "Xuân về măng mọc lô nhô nối đời\n",
      "Trưa hè nhịp võng  à ơi\n",
      "Thương cha nhớ mẹ cuộc người vì con\n"
     ]
    }
   ],
   "source": [
    "url = \"http://lucbat.com/news.php?id=22934\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    div_element = soup.find('div', {'class': 'protop_list'})\n",
    "\n",
    "    if div_element:\n",
    "        p_elements = div_element.find_all('p', {'class': 'MsoNormal'})\n",
    "\n",
    "        content = \"\\n\".join(re.sub(r'[^\\w\\s]', '', p.get_text(strip=True)) for p in p_elements)\n",
    "\n",
    "        start_pattern = re.compile(r'\\n\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\n')\n",
    "        end_pattern = re.compile(r'\\n\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+\\n')\n",
    "\n",
    "        start_match = start_pattern.search(content)\n",
    "        end_match = end_pattern.findall(content)\n",
    "\n",
    "        if start_match and end_match:\n",
    "            start_index = start_match.start()\n",
    "            end_index = content.rfind(end_match[-1]) + len(end_match[-1])\n",
    "\n",
    "            extracted_content = content[start_index:end_index].strip()\n",
    "            print(extracted_content)\n",
    "        else:\n",
    "            print(\"Patterns not found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
